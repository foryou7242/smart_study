{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Variable in module tensorflow.python.ops.variables:\n",
      "\n",
      "class Variable(tensorflow.python.training.checkpointable.base.CheckpointableBase)\n",
      " |  See the @{$variables$Variables How To} for a high level overview.\n",
      " |  \n",
      " |  A variable maintains state in the graph across calls to `run()`. You add a\n",
      " |  variable to the graph by constructing an instance of the class `Variable`.\n",
      " |  \n",
      " |  The `Variable()` constructor requires an initial value for the variable,\n",
      " |  which can be a `Tensor` of any type and shape. The initial value defines the\n",
      " |  type and shape of the variable. After construction, the type and shape of\n",
      " |  the variable are fixed. The value can be changed using one of the assign\n",
      " |  methods.\n",
      " |  \n",
      " |  If you want to change the shape of a variable later you have to use an\n",
      " |  `assign` Op with `validate_shape=False`.\n",
      " |  \n",
      " |  Just like any `Tensor`, variables created with `Variable()` can be used as\n",
      " |  inputs for other Ops in the graph. Additionally, all the operators\n",
      " |  overloaded for the `Tensor` class are carried over to variables, so you can\n",
      " |  also add nodes to the graph by just doing arithmetic on variables.\n",
      " |  \n",
      " |  ```python\n",
      " |  import tensorflow as tf\n",
      " |  \n",
      " |  # Create a variable.\n",
      " |  w = tf.Variable(<initial-value>, name=<optional-name>)\n",
      " |  \n",
      " |  # Use the variable in the graph like any Tensor.\n",
      " |  y = tf.matmul(w, ...another variable or tensor...)\n",
      " |  \n",
      " |  # The overloaded operators are available too.\n",
      " |  z = tf.sigmoid(w + y)\n",
      " |  \n",
      " |  # Assign a new value to the variable with `assign()` or a related method.\n",
      " |  w.assign(w + 1.0)\n",
      " |  w.assign_add(1.0)\n",
      " |  ```\n",
      " |  \n",
      " |  When you launch the graph, variables have to be explicitly initialized before\n",
      " |  you can run Ops that use their value. You can initialize a variable by\n",
      " |  running its *initializer op*, restoring the variable from a save file, or\n",
      " |  simply running an `assign` Op that assigns a value to the variable. In fact,\n",
      " |  the variable *initializer op* is just an `assign` Op that assigns the\n",
      " |  variable's initial value to the variable itself.\n",
      " |  \n",
      " |  ```python\n",
      " |  # Launch the graph in a session.\n",
      " |  with tf.Session() as sess:\n",
      " |      # Run the variable initializer.\n",
      " |      sess.run(w.initializer)\n",
      " |      # ...you now can run ops that use the value of 'w'...\n",
      " |  ```\n",
      " |  \n",
      " |  The most common initialization pattern is to use the convenience function\n",
      " |  `global_variables_initializer()` to add an Op to the graph that initializes\n",
      " |  all the variables. You then run that Op after launching the graph.\n",
      " |  \n",
      " |  ```python\n",
      " |  # Add an Op to initialize global variables.\n",
      " |  init_op = tf.global_variables_initializer()\n",
      " |  \n",
      " |  # Launch the graph in a session.\n",
      " |  with tf.Session() as sess:\n",
      " |      # Run the Op that initializes global variables.\n",
      " |      sess.run(init_op)\n",
      " |      # ...you can now run any Op that uses variable values...\n",
      " |  ```\n",
      " |  \n",
      " |  If you need to create a variable with an initial value dependent on another\n",
      " |  variable, use the other variable's `initialized_value()`. This ensures that\n",
      " |  variables are initialized in the right order.\n",
      " |  \n",
      " |  All variables are automatically collected in the graph where they are\n",
      " |  created. By default, the constructor adds the new variable to the graph\n",
      " |  collection `GraphKeys.GLOBAL_VARIABLES`. The convenience function\n",
      " |  `global_variables()` returns the contents of that collection.\n",
      " |  \n",
      " |  When building a machine learning model it is often convenient to distinguish\n",
      " |  between variables holding the trainable model parameters and other variables\n",
      " |  such as a `global step` variable used to count training steps. To make this\n",
      " |  easier, the variable constructor supports a `trainable=<bool>` parameter. If\n",
      " |  `True`, the new variable is also added to the graph collection\n",
      " |  `GraphKeys.TRAINABLE_VARIABLES`. The convenience function\n",
      " |  `trainable_variables()` returns the contents of this collection. The\n",
      " |  various `Optimizer` classes use this collection as the default list of\n",
      " |  variables to optimize.\n",
      " |  \n",
      " |  WARNING: tf.Variable objects have a non-intuitive memory model. A Variable is\n",
      " |  represented internally as a mutable Tensor which can non-deterministically\n",
      " |  alias other Tensors in a graph. The set of operations which consume a Variable\n",
      " |  and can lead to aliasing is undetermined and can change across TensorFlow\n",
      " |  versions. Avoid writing code which relies on the value of a Variable either\n",
      " |  changing or not changing as other operations happen. For example, using\n",
      " |  Variable objects or simple functions thereof as predicates in a `tf.cond` is\n",
      " |  dangerous and error-prone:\n",
      " |  \n",
      " |  ```\n",
      " |  v = tf.Variable(True)\n",
      " |  tf.cond(v, lambda: v.assign(False), my_false_fn)  # Note: this is broken.\n",
      " |  ```\n",
      " |  \n",
      " |  Here replacing tf.Variable with tf.contrib.eager.Variable will fix any\n",
      " |  nondeterminism issues.\n",
      " |  \n",
      " |  To use the replacement for variables which does\n",
      " |  not have these issues:\n",
      " |  \n",
      " |  * Replace `tf.Variable` with `tf.contrib.eager.Variable`;\n",
      " |  * Call `tf.get_variable_scope().set_use_resource(True)` inside a\n",
      " |    `tf.variable_scope` before the `tf.get_variable()` call.\n",
      " |  \n",
      " |  @compatibility(eager)\n",
      " |  `tf.Variable` is not compatible with eager execution.  Use\n",
      " |  `tf.contrib.eager.Variable` instead which is compatible with both eager\n",
      " |  execution and graph construction.  See [the TensorFlow Eager Execution\n",
      " |  guide](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/g3doc/guide.md#variables-and-optimizers)\n",
      " |  for details on how variables work in eager execution.\n",
      " |  @end_compatibility\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Variable\n",
      " |      tensorflow.python.training.checkpointable.base.CheckpointableBase\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __abs__ = _run_op(a, *args)\n",
      " |      Computes the absolute value of a tensor.\n",
      " |      \n",
      " |      Given a tensor `x` of complex numbers, this operation returns a tensor of type\n",
      " |      `float32` or `float64` that is the absolute value of each element in `x`. All\n",
      " |      elements in `x` must be complex numbers of the form \\\\(a + bj\\\\). The\n",
      " |      absolute value is computed as \\\\( \\sqrt{a^2 + b^2}\\\\).  For example:\n",
      " |      ```python\n",
      " |      x = tf.constant([[-2.25 + 4.75j], [-3.25 + 5.75j]])\n",
      " |      tf.abs(x)  # [5.25594902, 6.60492229]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` or `SparseTensor` of type `float16`, `float32`, `float64`,\n",
      " |          `int32`, `int64`, `complex64` or `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` or `SparseTensor` the same size and type as `x` with absolute\n",
      " |          values.\n",
      " |        Note, for `complex64` or `complex128` input, the returned `Tensor` will be\n",
      " |          of type `float32` or `float64`, respectively.\n",
      " |  \n",
      " |  __add__ = _run_op(a, *args)\n",
      " |      Returns x + y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.add` supports broadcasting. `AddN` does not. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`, `string`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __and__ = _run_op(a, *args)\n",
      " |      Returns the truth value of x AND y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.logical_and` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __div__ = _run_op(a, *args)\n",
      " |      Divide two values using Python 2 semantics. Used for Tensor.__div__.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      Returns:\n",
      " |        `x / y` returns the quotient of x and y.\n",
      " |  \n",
      " |  __floordiv__ = _run_op(a, *args)\n",
      " |      Divides `x / y` elementwise, rounding toward the most negative integer.\n",
      " |      \n",
      " |      The same as `tf.div(x,y)` for integers, but uses `tf.floor(tf.div(x,y))` for\n",
      " |      floating point arguments so that the result is always an integer (though\n",
      " |      possibly an integer represented as floating point).  This op is generated by\n",
      " |      `x // y` floor division in Python 3 and in Python 2.7 with\n",
      " |      `from __future__ import division`.\n",
      " |      \n",
      " |      Note that for efficiency, `floordiv` uses C semantics for negative numbers\n",
      " |      (unlike Python and Numpy).\n",
      " |      \n",
      " |      `x` and `y` must have the same type, and the result will have the same type\n",
      " |      as well.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` rounded down (except possibly towards zero for negative integers).\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If the inputs are complex.\n",
      " |  \n",
      " |  __ge__ = _run_op(a, *args)\n",
      " |      Returns the truth value of (x >= y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.greater_equal` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __getitem__ = _SliceHelperVar(var, slice_spec)\n",
      " |      Creates a slice helper object given a variable.\n",
      " |      \n",
      " |      This allows creating a sub-tensor from part of the current contents\n",
      " |      of a variable. See @{tf.Tensor.__getitem__} for detailed examples\n",
      " |      of slicing.\n",
      " |      \n",
      " |      This function in addition also allows assignment to a sliced range.\n",
      " |      This is similar to `__setitem__` functionality in Python. However,\n",
      " |      the syntax is different so that the user can capture the assignment\n",
      " |      operation for grouping or passing to `sess.run()`.\n",
      " |      For example,\n",
      " |      \n",
      " |      ```python\n",
      " |      import tensorflow as tf\n",
      " |      A = tf.Variable([[1,2,3], [4,5,6], [7,8,9]], dtype=tf.float32)\n",
      " |      with tf.Session() as sess:\n",
      " |        sess.run(tf.global_variables_initializer())\n",
      " |        print(sess.run(A[:2, :2]))  # => [[1,2], [4,5]]\n",
      " |      \n",
      " |        op = A[:2,:2].assign(22. * tf.ones((2, 2)))\n",
      " |        print(sess.run(op))  # => [[22, 22, 3], [22, 22, 6], [7,8,9]]\n",
      " |      ```\n",
      " |      \n",
      " |      Note that assignments currently do not support NumPy broadcasting\n",
      " |      semantics.\n",
      " |      \n",
      " |      Args:\n",
      " |        var: An `ops.Variable` object.\n",
      " |        slice_spec: The arguments to `Tensor.__getitem__`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The appropriate slice of \"tensor\", based on \"slice_spec\".\n",
      " |        As an operator. The operator also has a `assign()` method\n",
      " |        that can be used to generate an assignment operator.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If a slice range is negative size.\n",
      " |        TypeError: If the slice indices aren't int, slice, or Ellipsis.\n",
      " |  \n",
      " |  __gt__ = _run_op(a, *args)\n",
      " |      Returns the truth value of (x > y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.greater` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __iadd__(self, other)\n",
      " |  \n",
      " |  __idiv__(self, other)\n",
      " |  \n",
      " |  __imul__(self, other)\n",
      " |  \n",
      " |  __init__(self, initial_value=None, trainable=True, collections=None, validate_shape=True, caching_device=None, name=None, variable_def=None, dtype=None, expected_shape=None, import_scope=None, constraint=None)\n",
      " |      Creates a new variable with value `initial_value`.\n",
      " |      \n",
      " |      The new variable is added to the graph collections listed in `collections`,\n",
      " |      which defaults to `[GraphKeys.GLOBAL_VARIABLES]`.\n",
      " |      \n",
      " |      If `trainable` is `True` the variable is also added to the graph collection\n",
      " |      `GraphKeys.TRAINABLE_VARIABLES`.\n",
      " |      \n",
      " |      This constructor creates both a `variable` Op and an `assign` Op to set the\n",
      " |      variable to its initial value.\n",
      " |      \n",
      " |      Args:\n",
      " |        initial_value: A `Tensor`, or Python object convertible to a `Tensor`,\n",
      " |          which is the initial value for the Variable. The initial value must have\n",
      " |          a shape specified unless `validate_shape` is set to False. Can also be a\n",
      " |          callable with no argument that returns the initial value when called. In\n",
      " |          that case, `dtype` must be specified. (Note that initializer functions\n",
      " |          from init_ops.py must first be bound to a shape before being used here.)\n",
      " |        trainable: If `True`, the default, also adds the variable to the graph\n",
      " |          collection `GraphKeys.TRAINABLE_VARIABLES`. This collection is used as\n",
      " |          the default list of variables to use by the `Optimizer` classes.\n",
      " |        collections: List of graph collections keys. The new variable is added to\n",
      " |          these collections. Defaults to `[GraphKeys.GLOBAL_VARIABLES]`.\n",
      " |        validate_shape: If `False`, allows the variable to be initialized with a\n",
      " |          value of unknown shape. If `True`, the default, the shape of\n",
      " |          `initial_value` must be known.\n",
      " |        caching_device: Optional device string describing where the Variable\n",
      " |          should be cached for reading.  Defaults to the Variable's device.\n",
      " |          If not `None`, caches on another device.  Typical use is to cache\n",
      " |          on the device where the Ops using the Variable reside, to deduplicate\n",
      " |          copying through `Switch` and other conditional statements.\n",
      " |        name: Optional name for the variable. Defaults to `'Variable'` and gets\n",
      " |          uniquified automatically.\n",
      " |        variable_def: `VariableDef` protocol buffer. If not `None`, recreates\n",
      " |          the Variable object with its contents, referencing the variable's nodes\n",
      " |          in the graph, which must already exist. The graph is not changed.\n",
      " |          `variable_def` and the other arguments are mutually exclusive.\n",
      " |        dtype: If set, initial_value will be converted to the given type.\n",
      " |          If `None`, either the datatype will be kept (if `initial_value` is\n",
      " |          a Tensor), or `convert_to_tensor` will decide.\n",
      " |        expected_shape: A TensorShape. If set, initial_value is expected\n",
      " |          to have this shape.\n",
      " |        import_scope: Optional `string`. Name scope to add to the\n",
      " |          `Variable.` Only used when initializing from protocol buffer.\n",
      " |        constraint: An optional projection function to be applied to the variable\n",
      " |          after being updated by an `Optimizer` (e.g. used to implement norm\n",
      " |          constraints or value constraints for layer weights). The function must\n",
      " |          take as input the unprojected Tensor representing the value of the\n",
      " |          variable and return the Tensor for the projected value\n",
      " |          (which must have the same shape). Constraints are not safe to\n",
      " |          use when doing asynchronous distributed training.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If both `variable_def` and initial_value are specified.\n",
      " |        ValueError: If the initial value is not specified, or does not have a\n",
      " |          shape and `validate_shape` is `True`.\n",
      " |        RuntimeError: If eager execution is enabled.\n",
      " |      \n",
      " |      @compatibility(eager)\n",
      " |      `tf.Variable` is not compatible with eager execution.  Use\n",
      " |      `tfe.Variable` instead which is compatible with both eager execution\n",
      " |      and graph construction.  See [the TensorFlow Eager Execution\n",
      " |      guide](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/g3doc/guide.md#variables-and-optimizers)\n",
      " |      for details on how variables work in eager execution.\n",
      " |      @end_compatibility\n",
      " |  \n",
      " |  __invert__ = _run_op(a, *args)\n",
      " |      Returns the truth value of NOT x element-wise.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __ipow__(self, other)\n",
      " |  \n",
      " |  __irealdiv__(self, other)\n",
      " |  \n",
      " |  __isub__(self, other)\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Dummy method to prevent iteration. Do not call.\n",
      " |      \n",
      " |      NOTE(mrry): If we register __getitem__ as an overloaded operator,\n",
      " |      Python will valiantly attempt to iterate over the variable's Tensor from 0\n",
      " |      to infinity.  Declaring this method prevents this unintended behavior.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: when invoked.\n",
      " |  \n",
      " |  __itruediv__(self, other)\n",
      " |  \n",
      " |  __le__ = _run_op(a, *args)\n",
      " |      Returns the truth value of (x <= y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.less_equal` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __lt__ = _run_op(a, *args)\n",
      " |      Returns the truth value of (x < y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.less` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __matmul__ = _run_op(a, *args)\n",
      " |      Multiplies matrix `a` by matrix `b`, producing `a` * `b`.\n",
      " |      \n",
      " |      The inputs must, following any transpositions, be tensors of rank >= 2\n",
      " |      where the inner 2 dimensions specify valid matrix multiplication arguments,\n",
      " |      and any further outer dimensions match.\n",
      " |      \n",
      " |      Both matrices must be of the same type. The supported types are:\n",
      " |      `float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.\n",
      " |      \n",
      " |      Either matrix can be transposed or adjointed (conjugated and transposed) on\n",
      " |      the fly by setting one of the corresponding flag to `True`. These are `False`\n",
      " |      by default.\n",
      " |      \n",
      " |      If one or both of the matrices contain a lot of zeros, a more efficient\n",
      " |      multiplication algorithm can be used by setting the corresponding\n",
      " |      `a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.\n",
      " |      This optimization is only available for plain matrices (rank-2 tensors) with\n",
      " |      datatypes `bfloat16` or `float32`.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      # 2-D tensor `a`\n",
      " |      # [[1, 2, 3],\n",
      " |      #  [4, 5, 6]]\n",
      " |      a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
      " |      \n",
      " |      # 2-D tensor `b`\n",
      " |      # [[ 7,  8],\n",
      " |      #  [ 9, 10],\n",
      " |      #  [11, 12]]\n",
      " |      b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n",
      " |      \n",
      " |      # `a` * `b`\n",
      " |      # [[ 58,  64],\n",
      " |      #  [139, 154]]\n",
      " |      c = tf.matmul(a, b)\n",
      " |      \n",
      " |      \n",
      " |      # 3-D tensor `a`\n",
      " |      # [[[ 1,  2,  3],\n",
      " |      #   [ 4,  5,  6]],\n",
      " |      #  [[ 7,  8,  9],\n",
      " |      #   [10, 11, 12]]]\n",
      " |      a = tf.constant(np.arange(1, 13, dtype=np.int32),\n",
      " |                      shape=[2, 2, 3])\n",
      " |      \n",
      " |      # 3-D tensor `b`\n",
      " |      # [[[13, 14],\n",
      " |      #   [15, 16],\n",
      " |      #   [17, 18]],\n",
      " |      #  [[19, 20],\n",
      " |      #   [21, 22],\n",
      " |      #   [23, 24]]]\n",
      " |      b = tf.constant(np.arange(13, 25, dtype=np.int32),\n",
      " |                      shape=[2, 3, 2])\n",
      " |      \n",
      " |      # `a` * `b`\n",
      " |      # [[[ 94, 100],\n",
      " |      #   [229, 244]],\n",
      " |      #  [[508, 532],\n",
      " |      #   [697, 730]]]\n",
      " |      c = tf.matmul(a, b)\n",
      " |      \n",
      " |      # Since python >= 3.5 the @ operator is supported (see PEP 465).\n",
      " |      # In TensorFlow, it simply calls the `tf.matmul()` function, so the\n",
      " |      # following lines are equivalent:\n",
      " |      d = a @ b @ [[10.], [11.]]\n",
      " |      d = tf.matmul(tf.matmul(a, b), [[10.], [11.]])\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        a: `Tensor` of type `float16`, `float32`, `float64`, `int32`, `complex64`,\n",
      " |          `complex128` and rank > 1.\n",
      " |        b: `Tensor` with same type and rank as `a`.\n",
      " |        transpose_a: If `True`, `a` is transposed before multiplication.\n",
      " |        transpose_b: If `True`, `b` is transposed before multiplication.\n",
      " |        adjoint_a: If `True`, `a` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        adjoint_b: If `True`, `b` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        a_is_sparse: If `True`, `a` is treated as a sparse matrix.\n",
      " |        b_is_sparse: If `True`, `b` is treated as a sparse matrix.\n",
      " |        name: Name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of the same type as `a` and `b` where each inner-most matrix is\n",
      " |        the product of the corresponding matrices in `a` and `b`, e.g. if all\n",
      " |        transpose or adjoint attributes are `False`:\n",
      " |      \n",
      " |        `output`[..., i, j] = sum_k (`a`[..., i, k] * `b`[..., k, j]),\n",
      " |        for all indices i, j.\n",
      " |      \n",
      " |        Note: This is matrix product, not element-wise product.\n",
      " |      \n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If transpose_a and adjoint_a, or transpose_b and adjoint_b\n",
      " |          are both set to True.\n",
      " |  \n",
      " |  __mod__ = _run_op(a, *args)\n",
      " |      Returns element-wise remainder of division. When `x < 0` xor `y < 0` is\n",
      " |      \n",
      " |      true, this follows Python semantics in that the result here is consistent\n",
      " |      with a flooring divide. E.g. `floor(x / y) * y + mod(x, y) = x`.\n",
      " |      \n",
      " |      *NOTE*: `FloorMod` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `int32`, `int64`, `bfloat16`, `half`, `float32`, `float64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __mul__ = _run_op(a, *args)\n",
      " |      Dispatches cwise mul for \"Dense*Dense\" and \"Dense*Sparse\".\n",
      " |  \n",
      " |  __neg__ = _run_op(a, *args)\n",
      " |      Computes numerical negative value element-wise.\n",
      " |      \n",
      " |      I.e., \\\\(y = -x\\\\).\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int32`, `int64`, `complex64`, `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __or__ = _run_op(a, *args)\n",
      " |      Returns the truth value of x OR y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.logical_or` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __pow__ = _run_op(a, *args)\n",
      " |      Computes the power of one value to another.\n",
      " |      \n",
      " |      Given a tensor `x` and a tensor `y`, this operation computes \\\\(x^y\\\\) for\n",
      " |      corresponding elements in `x` and `y`. For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([[2, 2], [3, 3]])\n",
      " |      y = tf.constant([[8, 16], [2, 3]])\n",
      " |      tf.pow(x, y)  # [[256, 65536], [9, 27]]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
      " |         `complex64`, or `complex128`.\n",
      " |        y: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
      " |         `complex64`, or `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`.\n",
      " |  \n",
      " |  __radd__ = _run_op(a, *args)\n",
      " |      Returns x + y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.add` supports broadcasting. `AddN` does not. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`, `string`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __rand__ = _run_op(a, *args)\n",
      " |      Returns the truth value of x AND y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.logical_and` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __rdiv__ = _run_op(a, *args)\n",
      " |      Divide two values using Python 2 semantics. Used for Tensor.__div__.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      Returns:\n",
      " |        `x / y` returns the quotient of x and y.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __rfloordiv__ = _run_op(a, *args)\n",
      " |      Divides `x / y` elementwise, rounding toward the most negative integer.\n",
      " |      \n",
      " |      The same as `tf.div(x,y)` for integers, but uses `tf.floor(tf.div(x,y))` for\n",
      " |      floating point arguments so that the result is always an integer (though\n",
      " |      possibly an integer represented as floating point).  This op is generated by\n",
      " |      `x // y` floor division in Python 3 and in Python 2.7 with\n",
      " |      `from __future__ import division`.\n",
      " |      \n",
      " |      Note that for efficiency, `floordiv` uses C semantics for negative numbers\n",
      " |      (unlike Python and Numpy).\n",
      " |      \n",
      " |      `x` and `y` must have the same type, and the result will have the same type\n",
      " |      as well.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` rounded down (except possibly towards zero for negative integers).\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If the inputs are complex.\n",
      " |  \n",
      " |  __rmatmul__ = _run_op(a, *args)\n",
      " |      Multiplies matrix `a` by matrix `b`, producing `a` * `b`.\n",
      " |      \n",
      " |      The inputs must, following any transpositions, be tensors of rank >= 2\n",
      " |      where the inner 2 dimensions specify valid matrix multiplication arguments,\n",
      " |      and any further outer dimensions match.\n",
      " |      \n",
      " |      Both matrices must be of the same type. The supported types are:\n",
      " |      `float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.\n",
      " |      \n",
      " |      Either matrix can be transposed or adjointed (conjugated and transposed) on\n",
      " |      the fly by setting one of the corresponding flag to `True`. These are `False`\n",
      " |      by default.\n",
      " |      \n",
      " |      If one or both of the matrices contain a lot of zeros, a more efficient\n",
      " |      multiplication algorithm can be used by setting the corresponding\n",
      " |      `a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.\n",
      " |      This optimization is only available for plain matrices (rank-2 tensors) with\n",
      " |      datatypes `bfloat16` or `float32`.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      # 2-D tensor `a`\n",
      " |      # [[1, 2, 3],\n",
      " |      #  [4, 5, 6]]\n",
      " |      a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
      " |      \n",
      " |      # 2-D tensor `b`\n",
      " |      # [[ 7,  8],\n",
      " |      #  [ 9, 10],\n",
      " |      #  [11, 12]]\n",
      " |      b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n",
      " |      \n",
      " |      # `a` * `b`\n",
      " |      # [[ 58,  64],\n",
      " |      #  [139, 154]]\n",
      " |      c = tf.matmul(a, b)\n",
      " |      \n",
      " |      \n",
      " |      # 3-D tensor `a`\n",
      " |      # [[[ 1,  2,  3],\n",
      " |      #   [ 4,  5,  6]],\n",
      " |      #  [[ 7,  8,  9],\n",
      " |      #   [10, 11, 12]]]\n",
      " |      a = tf.constant(np.arange(1, 13, dtype=np.int32),\n",
      " |                      shape=[2, 2, 3])\n",
      " |      \n",
      " |      # 3-D tensor `b`\n",
      " |      # [[[13, 14],\n",
      " |      #   [15, 16],\n",
      " |      #   [17, 18]],\n",
      " |      #  [[19, 20],\n",
      " |      #   [21, 22],\n",
      " |      #   [23, 24]]]\n",
      " |      b = tf.constant(np.arange(13, 25, dtype=np.int32),\n",
      " |                      shape=[2, 3, 2])\n",
      " |      \n",
      " |      # `a` * `b`\n",
      " |      # [[[ 94, 100],\n",
      " |      #   [229, 244]],\n",
      " |      #  [[508, 532],\n",
      " |      #   [697, 730]]]\n",
      " |      c = tf.matmul(a, b)\n",
      " |      \n",
      " |      # Since python >= 3.5 the @ operator is supported (see PEP 465).\n",
      " |      # In TensorFlow, it simply calls the `tf.matmul()` function, so the\n",
      " |      # following lines are equivalent:\n",
      " |      d = a @ b @ [[10.], [11.]]\n",
      " |      d = tf.matmul(tf.matmul(a, b), [[10.], [11.]])\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        a: `Tensor` of type `float16`, `float32`, `float64`, `int32`, `complex64`,\n",
      " |          `complex128` and rank > 1.\n",
      " |        b: `Tensor` with same type and rank as `a`.\n",
      " |        transpose_a: If `True`, `a` is transposed before multiplication.\n",
      " |        transpose_b: If `True`, `b` is transposed before multiplication.\n",
      " |        adjoint_a: If `True`, `a` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        adjoint_b: If `True`, `b` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        a_is_sparse: If `True`, `a` is treated as a sparse matrix.\n",
      " |        b_is_sparse: If `True`, `b` is treated as a sparse matrix.\n",
      " |        name: Name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of the same type as `a` and `b` where each inner-most matrix is\n",
      " |        the product of the corresponding matrices in `a` and `b`, e.g. if all\n",
      " |        transpose or adjoint attributes are `False`:\n",
      " |      \n",
      " |        `output`[..., i, j] = sum_k (`a`[..., i, k] * `b`[..., k, j]),\n",
      " |        for all indices i, j.\n",
      " |      \n",
      " |        Note: This is matrix product, not element-wise product.\n",
      " |      \n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If transpose_a and adjoint_a, or transpose_b and adjoint_b\n",
      " |          are both set to True.\n",
      " |  \n",
      " |  __rmod__ = _run_op(a, *args)\n",
      " |      Returns element-wise remainder of division. When `x < 0` xor `y < 0` is\n",
      " |      \n",
      " |      true, this follows Python semantics in that the result here is consistent\n",
      " |      with a flooring divide. E.g. `floor(x / y) * y + mod(x, y) = x`.\n",
      " |      \n",
      " |      *NOTE*: `FloorMod` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `int32`, `int64`, `bfloat16`, `half`, `float32`, `float64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __rmul__ = _run_op(a, *args)\n",
      " |      Dispatches cwise mul for \"Dense*Dense\" and \"Dense*Sparse\".\n",
      " |  \n",
      " |  __ror__ = _run_op(a, *args)\n",
      " |      Returns the truth value of x OR y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `math.logical_or` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __rpow__ = _run_op(a, *args)\n",
      " |      Computes the power of one value to another.\n",
      " |      \n",
      " |      Given a tensor `x` and a tensor `y`, this operation computes \\\\(x^y\\\\) for\n",
      " |      corresponding elements in `x` and `y`. For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([[2, 2], [3, 3]])\n",
      " |      y = tf.constant([[8, 16], [2, 3]])\n",
      " |      tf.pow(x, y)  # [[256, 65536], [9, 27]]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
      " |         `complex64`, or `complex128`.\n",
      " |        y: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,\n",
      " |         `complex64`, or `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`.\n",
      " |  \n",
      " |  __rsub__ = _run_op(a, *args)\n",
      " |      Returns x - y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `Subtract` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __rtruediv__ = _run_op(a, *args)\n",
      " |  \n",
      " |  __rxor__ = _run_op(a, *args)\n",
      " |      x ^ y = (x | y) & ~(x & y).\n",
      " |  \n",
      " |  __sub__ = _run_op(a, *args)\n",
      " |      Returns x - y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `Subtract` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __truediv__ = _run_op(a, *args)\n",
      " |  \n",
      " |  __xor__ = _run_op(a, *args)\n",
      " |      x ^ y = (x | y) & ~(x & y).\n",
      " |  \n",
      " |  assign(self, value, use_locking=False)\n",
      " |      Assigns a new value to the variable.\n",
      " |      \n",
      " |      This is essentially a shortcut for `assign(self, value)`.\n",
      " |      \n",
      " |      Args:\n",
      " |        value: A `Tensor`. The new value for this variable.\n",
      " |        use_locking: If `True`, use locking during the assignment.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the assignment has completed.\n",
      " |  \n",
      " |  assign_add(self, delta, use_locking=False)\n",
      " |      Adds a value to this variable.\n",
      " |      \n",
      " |       This is essentially a shortcut for `assign_add(self, delta)`.\n",
      " |      \n",
      " |      Args:\n",
      " |        delta: A `Tensor`. The value to add to this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the addition has completed.\n",
      " |  \n",
      " |  assign_sub(self, delta, use_locking=False)\n",
      " |      Subtracts a value from this variable.\n",
      " |      \n",
      " |      This is essentially a shortcut for `assign_sub(self, delta)`.\n",
      " |      \n",
      " |      Args:\n",
      " |        delta: A `Tensor`. The value to subtract from this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the subtraction has completed.\n",
      " |  \n",
      " |  count_up_to(self, limit)\n",
      " |      Increments this variable until it reaches `limit`.\n",
      " |      \n",
      " |      When that Op is run it tries to increment the variable by `1`. If\n",
      " |      incrementing the variable would bring it above `limit` then the Op raises\n",
      " |      the exception `OutOfRangeError`.\n",
      " |      \n",
      " |      If no error is raised, the Op outputs the value of the variable before\n",
      " |      the increment.\n",
      " |      \n",
      " |      This is essentially a shortcut for `count_up_to(self, limit)`.\n",
      " |      \n",
      " |      Args:\n",
      " |        limit: value at which incrementing the variable raises an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the variable value before the increment. If no\n",
      " |        other Op modifies this variable, the values produced will all be\n",
      " |        distinct.\n",
      " |  \n",
      " |  eval(self, session=None)\n",
      " |      In a session, computes and returns the value of this variable.\n",
      " |      \n",
      " |      This is not a graph construction method, it does not add ops to the graph.\n",
      " |      \n",
      " |      This convenience method requires a session where the graph\n",
      " |      containing this variable has been launched. If no session is\n",
      " |      passed, the default session is used.  See @{tf.Session} for more\n",
      " |      information on launching a graph and on sessions.\n",
      " |      \n",
      " |      ```python\n",
      " |      v = tf.Variable([1, 2])\n",
      " |      init = tf.global_variables_initializer()\n",
      " |      \n",
      " |      with tf.Session() as sess:\n",
      " |          sess.run(init)\n",
      " |          # Usage passing the session explicitly.\n",
      " |          print(v.eval(sess))\n",
      " |          # Usage with the default session.  The 'with' block\n",
      " |          # above makes 'sess' the default session.\n",
      " |          print(v.eval())\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        session: The session to use to evaluate this variable. If\n",
      " |          none, the default session is used.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A numpy `ndarray` with a copy of the value of this variable.\n",
      " |  \n",
      " |  get_shape(self)\n",
      " |      Alias of Variable.shape.\n",
      " |  \n",
      " |  initialized_value(self)\n",
      " |      Returns the value of the initialized variable.\n",
      " |      \n",
      " |      You should use this instead of the variable itself to initialize another\n",
      " |      variable with a value that depends on the value of this variable.\n",
      " |      \n",
      " |      ```python\n",
      " |      # Initialize 'v' with a random tensor.\n",
      " |      v = tf.Variable(tf.truncated_normal([10, 40]))\n",
      " |      # Use `initialized_value` to guarantee that `v` has been\n",
      " |      # initialized before its value is used to initialize `w`.\n",
      " |      # The random values are picked only once.\n",
      " |      w = tf.Variable(v.initialized_value() * 2.0)\n",
      " |      ```\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` holding the value of this variable after its initializer\n",
      " |        has run.\n",
      " |  \n",
      " |  load(self, value, session=None)\n",
      " |      Load new value into this variable.\n",
      " |      \n",
      " |      Writes new value to variable's memory. Doesn't add ops to the graph.\n",
      " |      \n",
      " |      This convenience method requires a session where the graph\n",
      " |      containing this variable has been launched. If no session is\n",
      " |      passed, the default session is used.  See @{tf.Session} for more\n",
      " |      information on launching a graph and on sessions.\n",
      " |      \n",
      " |      ```python\n",
      " |      v = tf.Variable([1, 2])\n",
      " |      init = tf.global_variables_initializer()\n",
      " |      \n",
      " |      with tf.Session() as sess:\n",
      " |          sess.run(init)\n",
      " |          # Usage passing the session explicitly.\n",
      " |          v.load([2, 3], sess)\n",
      " |          print(v.eval(sess)) # prints [2 3]\n",
      " |          # Usage with the default session.  The 'with' block\n",
      " |          # above makes 'sess' the default session.\n",
      " |          v.load([3, 4], sess)\n",
      " |          print(v.eval()) # prints [3 4]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |          value: New variable value\n",
      " |          session: The session to use to evaluate this variable. If\n",
      " |            none, the default session is used.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: Session is not passed and no default session\n",
      " |  \n",
      " |  read_value(self)\n",
      " |      Returns the value of this variable, read in the current context.\n",
      " |      \n",
      " |      Can be different from value() if it's on another device, with control\n",
      " |      dependencies, etc.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` containing the value of the variable.\n",
      " |  \n",
      " |  scatter_sub(self, sparse_delta, use_locking=False)\n",
      " |      Subtracts `IndexedSlices` from this variable.\n",
      " |      \n",
      " |      This is essentially a shortcut for `scatter_sub(self, sparse_delta.indices,\n",
      " |      sparse_delta.values)`.\n",
      " |      \n",
      " |      Args:\n",
      " |        sparse_delta: `IndexedSlices` to be subtracted from this variable.\n",
      " |        use_locking: If `True`, use locking during the operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` that will hold the new value of this variable after\n",
      " |        the scattered subtraction has completed.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if `sparse_delta` is not an `IndexedSlices`.\n",
      " |  \n",
      " |  set_shape(self, shape)\n",
      " |      Overrides the shape for this variable.\n",
      " |      \n",
      " |      Args:\n",
      " |        shape: the `TensorShape` representing the overridden shape.\n",
      " |  \n",
      " |  to_proto(self, export_scope=None)\n",
      " |      Converts a `Variable` to a `VariableDef` protocol buffer.\n",
      " |      \n",
      " |      Args:\n",
      " |        export_scope: Optional `string`. Name scope to remove.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `VariableDef` protocol buffer, or `None` if the `Variable` is not\n",
      " |        in the specified name scope.\n",
      " |  \n",
      " |  value(self)\n",
      " |      Returns the last snapshot of this variable.\n",
      " |      \n",
      " |      You usually do not need to call this method as all ops that need the value\n",
      " |      of the variable call it automatically through a `convert_to_tensor()` call.\n",
      " |      \n",
      " |      Returns a `Tensor` which holds the value of the variable.  You can not\n",
      " |      assign a new value to this tensor as it is not a reference to the variable.\n",
      " |      \n",
      " |      To avoid copies, if the consumer of the returned value is on the same device\n",
      " |      as the variable, this actually returns the live value of the variable, not\n",
      " |      a copy.  Updates to the variable are seen by the consumer.  If the consumer\n",
      " |      is on a different device it will get a copy of the variable.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` containing the value of the variable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  from_proto(variable_def, import_scope=None)\n",
      " |      Returns a `Variable` object created from `variable_def`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  constraint\n",
      " |      Returns the constraint function associated with this variable.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The constraint function that was passed to the variable constructor.\n",
      " |        Can be `None` if no constraint was passed.\n",
      " |  \n",
      " |  device\n",
      " |      The device of this variable.\n",
      " |  \n",
      " |  dtype\n",
      " |      The `DType` of this variable.\n",
      " |  \n",
      " |  graph\n",
      " |      The `Graph` of this variable.\n",
      " |  \n",
      " |  initial_value\n",
      " |      Returns the Tensor used as the initial value for the variable.\n",
      " |      \n",
      " |      Note that this is different from `initialized_value()` which runs\n",
      " |      the op that initializes the variable before returning its value.\n",
      " |      This method returns the tensor that is used by the op that initializes\n",
      " |      the variable.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`.\n",
      " |  \n",
      " |  initializer\n",
      " |      The initializer operation for this variable.\n",
      " |  \n",
      " |  name\n",
      " |      The name of this variable.\n",
      " |  \n",
      " |  op\n",
      " |      The `Operation` of this variable.\n",
      " |  \n",
      " |  shape\n",
      " |      The `TensorShape` of this variable.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `TensorShape`.\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  SaveSliceInfo = <class 'tensorflow.python.ops.variables.Variable.SaveS...\n",
      " |      Information on how to save this Variable as a slice.\n",
      " |      \n",
      " |      Provides internal support for saving variables as slices of a larger\n",
      " |      variable.  This API is not public and is subject to change.\n",
      " |      \n",
      " |      Available properties:\n",
      " |      \n",
      " |      * full_name\n",
      " |      * full_shape\n",
      " |      * var_offset\n",
      " |      * var_shape\n",
      " |  \n",
      " |  __array_priority__ = 100\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.checkpointable.base.CheckpointableBase:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.Variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function arange in module numpy.core.multiarray:\n",
      "\n",
      "arange(...)\n",
      "    arange([start,] stop[, step,], dtype=None)\n",
      "    \n",
      "    Return evenly spaced values within a given interval.\n",
      "    \n",
      "    Values are generated within the half-open interval ``[start, stop)``\n",
      "    (in other words, the interval including `start` but excluding `stop`).\n",
      "    For integer arguments the function is equivalent to the Python built-in\n",
      "    `range <http://docs.python.org/lib/built-in-funcs.html>`_ function,\n",
      "    but returns an ndarray rather than a list.\n",
      "    \n",
      "    When using a non-integer step, such as 0.1, the results will often not\n",
      "    be consistent.  It is better to use ``linspace`` for these cases.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    start : number, optional\n",
      "        Start of interval.  The interval includes this value.  The default\n",
      "        start value is 0.\n",
      "    stop : number\n",
      "        End of interval.  The interval does not include this value, except\n",
      "        in some cases where `step` is not an integer and floating point\n",
      "        round-off affects the length of `out`.\n",
      "    step : number, optional\n",
      "        Spacing between values.  For any output `out`, this is the distance\n",
      "        between two adjacent values, ``out[i+1] - out[i]``.  The default\n",
      "        step size is 1.  If `step` is specified, `start` must also be given.\n",
      "    dtype : dtype\n",
      "        The type of the output array.  If `dtype` is not given, infer the data\n",
      "        type from the other input arguments.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    arange : ndarray\n",
      "        Array of evenly spaced values.\n",
      "    \n",
      "        For floating point arguments, the length of the result is\n",
      "        ``ceil((stop - start)/step)``.  Because of floating point overflow,\n",
      "        this rule may result in the last element of `out` being greater\n",
      "        than `stop`.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    linspace : Evenly spaced numbers with careful handling of endpoints.\n",
      "    ogrid: Arrays of evenly spaced numbers in N-dimensions.\n",
      "    mgrid: Grid-shaped arrays of evenly spaced numbers in N-dimensions.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> np.arange(3)\n",
      "    array([0, 1, 2])\n",
      "    >>> np.arange(3.0)\n",
      "    array([ 0.,  1.,  2.])\n",
      "    >>> np.arange(3,7)\n",
      "    array([3, 4, 5, 6])\n",
      "    >>> np.arange(3,7,2)\n",
      "    array([3, 5])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "help(np.arange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.   1.   2.   3.   4.\n",
      "   5.   6.   7.   8.   9.  10.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb5c9c82860>]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb5d030a1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "#simple hypothesis\n",
    "\n",
    "#H(x) = Wx\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "w = tf.Variable(-10, dtype=tf.float32,name='w')\n",
    "hypothesis = w * x\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y))\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "x1 = np.arange(-10,11,1,dtype=float)\n",
    "print(x1)\n",
    "\n",
    "first = -9\n",
    "rets = []\n",
    "for _ in range(21):\n",
    "    w.load( first, sess)\n",
    "    first += 1\n",
    "    ret = sess.run(cost, feed_dict={x:x1,y:x1})\n",
    "    rets.append(ret)\n",
    "rets\n",
    "plt.xlim([-11, 11])\n",
    "plt.ylim([-1, 5000])\n",
    "plt.xlabel('x')     \n",
    "plt.ylabel('y')\n",
    "#y = x0 * we + bi\n",
    "        \n",
    "plt.plot(x1,rets)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method load in module tensorflow.python.ops.variables:\n",
      "\n",
      "load(value, session=None) method of tensorflow.python.ops.variables.Variable instance\n",
      "    Load new value into this variable.\n",
      "    \n",
      "    Writes new value to variable's memory. Doesn't add ops to the graph.\n",
      "    \n",
      "    This convenience method requires a session where the graph\n",
      "    containing this variable has been launched. If no session is\n",
      "    passed, the default session is used.  See @{tf.Session} for more\n",
      "    information on launching a graph and on sessions.\n",
      "    \n",
      "    ```python\n",
      "    v = tf.Variable([1, 2])\n",
      "    init = tf.global_variables_initializer()\n",
      "    \n",
      "    with tf.Session() as sess:\n",
      "        sess.run(init)\n",
      "        # Usage passing the session explicitly.\n",
      "        v.load([2, 3], sess)\n",
      "        print(v.eval(sess)) # prints [2 3]\n",
      "        # Usage with the default session.  The 'with' block\n",
      "        # above makes 'sess' the default session.\n",
      "        v.load([3, 4], sess)\n",
      "        print(v.eval()) # prints [3 4]\n",
      "    ```\n",
      "    \n",
      "    Args:\n",
      "        value: New variable value\n",
      "        session: The session to use to evaluate this variable. If\n",
      "          none, the default session is used.\n",
      "    \n",
      "    Raises:\n",
      "        ValueError: Session is not passed and no default session\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(w.load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'w_152:0'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function assign in module tensorflow.python.ops.state_ops:\n",
      "\n",
      "assign(ref, value, validate_shape=None, use_locking=None, name=None)\n",
      "    Update 'ref' by assigning 'value' to it.\n",
      "    \n",
      "    This operation outputs a Tensor that holds the new value of 'ref' after\n",
      "      the value has been assigned. This makes it easier to chain operations\n",
      "      that need to use the reset value.\n",
      "    \n",
      "    Args:\n",
      "      ref: A mutable `Tensor`.\n",
      "        Should be from a `Variable` node. May be uninitialized.\n",
      "      value: A `Tensor`. Must have the same type as `ref`.\n",
      "        The value to be assigned to the variable.\n",
      "      validate_shape: An optional `bool`. Defaults to `True`.\n",
      "        If true, the operation will validate that the shape\n",
      "        of 'value' matches the shape of the Tensor being assigned to.  If false,\n",
      "        'ref' will take on the shape of 'value'.\n",
      "      use_locking: An optional `bool`. Defaults to `True`.\n",
      "        If True, the assignment will be protected by a lock;\n",
      "        otherwise the behavior is undefined, but may exhibit less contention.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor` that will hold the new value of 'ref' after\n",
      "        the assignment has completed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.assign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'a_9:0' shape=() dtype=int32_ref>\n",
      "-10\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable(-10, tf.float32,name='a')\n",
    "print(w)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(sess.run(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'a_10:0' shape=() dtype=int32_ref>\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a_10:0'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "w = tf.Variable(1, tf.float32,name='a')\n",
    "print(w)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(sess.run(w))\n",
    "w.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0, 378.0\n",
      "1.5999994277954102, 1.6799968481063843\n",
      "1.0399999618530273, 0.007466652896255255\n",
      "1.002666711807251, 3.31863084284123e-05\n",
      "1.0001777410507202, 1.473864017498272e-07\n",
      "1.000011920928955, 6.631732385464773e-10\n",
      "1.0000008344650269, 3.055333763768431e-12\n",
      "1.0000001192092896, 9.947598300641403e-14\n",
      "1.0, 0.0\n",
      "1.0, 0.0\n",
      "1.0, 0.0\n",
      "1.0, 0.0\n",
      "1.0, 0.0\n",
      "1.0, 0.0\n",
      "1.0, 0.0\n",
      "1.0, 0.0\n",
      "1.0, 0.0\n",
      "1.0, 0.0\n",
      "1.0, 0.0\n",
      "1.0, 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb5bd85f8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_data = [1,2,3]\n",
    "y_data = [1,2,3]\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "W = tf.Variable(10,dtype=tf.float32,name='w')\n",
    "\n",
    "hypothesis = X * W\n",
    "learning_rate = 0.1\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "gradient = tf.reduce_mean((W*X-Y)*X) * 2\n",
    "descent=W - learning_rate*gradient\n",
    "update = W.assign(descent)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "W_val = []\n",
    "cost_val = []\n",
    "\n",
    "\n",
    "for i in range(20):\n",
    "    cur_cost,cur_W = sess.run([cost,W], feed_dict={X:x_data,Y:y_data})\n",
    "    sess.run([update,W],feed_dict={X:x_data,Y:y_data})\n",
    "    print(f'{cur_W}, {cur_cost}')\n",
    "    cost_val.append(cur_cost)\n",
    "    W_val.append(cur_W)\n",
    "plt.plot(W_val,cost_val,'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(None, <tf.Variable 'w:0' shape=() dtype=float32_ref>),\n",
       " (None, <tf.Variable 'w_1:0' shape=() dtype=float32_ref>),\n",
       " (<tf.Tensor 'gradients_2/mul_8_grad/tuple/control_dependency_1:0' shape=() dtype=float32>,\n",
       "  <tf.Variable 'w_2:0' shape=() dtype=float32_ref>)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1,2,3]\n",
    "y_data = [1,2,3]\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "W = tf.Variable(5,dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37.333332, [(37.333336, 5.0)]]\n",
      "[2.4888866, [(2.4888866, 1.2666664)]]\n",
      "[0.16592591, [(0.16592591, 1.0177778)]]\n",
      "[0.011061668, [(0.011061668, 1.0011852)]]\n",
      "[0.00073742867, [(0.00073742867, 1.000079)]]\n",
      "[4.895528e-05, [(4.8955284e-05, 1.0000052)]]\n",
      "[3.0994415e-06, [(3.0994415e-06, 1.0000004)]]\n",
      "[0.0, [(0.0, 1.0)]]\n",
      "[0.0, [(0.0, 1.0)]]\n",
      "[0.0, [(0.0, 1.0)]]\n",
      "[0.0, [(0.0, 1.0)]]\n",
      "[0.0, [(0.0, 1.0)]]\n",
      "[0.0, [(0.0, 1.0)]]\n",
      "[0.0, [(0.0, 1.0)]]\n",
      "[0.0, [(0.0, 1.0)]]\n",
      "[0.0, [(0.0, 1.0)]]\n",
      "[0.0, [(0.0, 1.0)]]\n",
      "[0.0, [(0.0, 1.0)]]\n",
      "[0.0, [(0.0, 1.0)]]\n",
      "[0.0, [(0.0, 1.0)]]\n",
      "[1.2666664, 1.0177778, 1.0011852, 1.000079, 1.0000052, 1.0000004, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[0.33185121, 0.0014748968, 6.5550271e-06, 2.9132201e-08, 1.2839034e-10, 5.163277e-13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAD7JJREFUeJzt3U2MXeV9x/Hvr2NcWYiIKh7yYjPFVS2QpYSAbiFqrKYseEsXpukiRFEipZEsFihiAZLZpIuoSiS6qCKRWBbyIgtKNxhZCsGCZsGCUPlaUIwRjqYOkT1JZUMgURUr2OTfxVynl/HYc2Z85+0+3480mnuelzv/R4/n5+Nzz/VNVSFJasefrHYBkqSVZfBLUmMMfklqjMEvSY0x+CWpMQa/JDXG4Jekxhj8ktQYg1+SGrNhtQuYz+bNm+uGG25Y7TIkad04cuTI21U12WXsmgz+G264gX6/v9plSNK6keQXXcd6qUeSGmPwS1JjDH5JaozBL0mNMfglqTFr8q4eSWrJM6/M8Nih4/zyvbN88tpNPHL3jdx3y5Zl+3kGvyStomdemeHRp49y9twHAMy8d5ZHnz4KsGzh76UeSVpFjx06/sfQv+DsuQ947NDxZfuZBr8kraJfvnd2Ue2jYPBL0ir65LWbFtU+Cga/JK2iR+6+kU1XTXyobdNVEzxy943L9jN9cVeSVtGFF3C9q0eSGnLfLVuWNejn8lKPJDXG4Jekxhj8ktQYg1+SGmPwS1JjDH5JaozBL0mNMfglqTEGvyQ1xuCXpMYY/JLUmE7Bn+SeJMeTTCfZM0//riSvJXk1ST/Jzq5zJUkra8HgTzIBPA7cC+wAvpxkx5xh/wHcXFWfAf4ReGIRcyVJK6jLGf9twHRVnaiq94GngF3DA6rqf6uqBodXA9V1riRpZXUJ/i3AyaHjU4O2D0ny90neBH7E7Fl/57mD+bsHl4n6Z86c6VK7JGkJRvbiblUdqKqbgPuAby9h/r6q6lVVb3JyclRlSZLm6BL8M8D1Q8dbB23zqqoXgb9IsnmxcyVJy69L8B8GtifZlmQjcD9wcHhAkr9MksHjW4E/Bd7pMleStLIW/OjFqjqf5EHgEDAB7K+qY0keGPTvBf4B+FqSc8BZ4EuDF3vnnbtMa5EkdZD/vxln7ej1etXv91e7DElaN5Icqapel7G+c1eSGmPwS1JjDH5JaozBL0mNMfglqTEGvyQ1xuCXpMYY/JLUGINfkhpj8EtSYwx+SWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1BiDX5IaY/BLUmMMfklqjMEvSY3pFPxJ7klyPMl0kj3z9H8lyWtJjiZ5KcnNQ31vDdpfTeInqEvSKtuw0IAkE8DjwJ3AKeBwkoNV9cbQsJ8Dn6+qd5PcC+wDbh/qv6Oq3h5h3ZKkJepyxn8bMF1VJ6rqfeApYNfwgKp6qareHRy+DGwdbZmSpFHpEvxbgJNDx6cGbZfyDeDHQ8cFvJDkSJLdiy9RkjRKC17qWYwkdzAb/DuHmndW1UyS64Dnk7xZVS/OM3c3sBtgampqlGVJkoZ0OeOfAa4fOt46aPuQJJ8GngB2VdU7F9qrambw/TRwgNlLRxepqn1V1auq3uTkZPcVSJIWpUvwHwa2J9mWZCNwP3BweECSKeBp4KtV9bOh9quTXHPhMXAX8PqoipckLd6Cl3qq6nySB4FDwASwv6qOJXlg0L8X+BbwUeD7SQDOV1UP+BhwYNC2AXiyqp5blpVIkjpJVa12DRfp9XrV73vLvyR1leTI4IR7Qb5zV5IaY/BLUmMMfklqjMEvSY0x+CWpMQa/JDXG4Jekxhj8ktQYg1+SGmPwS1JjDH5JaozBL0mNMfglqTEGvyQ1xuCXpMYY/JLUGINfkhpj8EtSYwx+SWqMwS9JjTH4JakxnYI/yT1JjieZTrJnnv6vJHktydEkLyW5uetcSdLKWjD4k0wAjwP3AjuALyfZMWfYz4HPV9WngG8D+xYxV5K0grqc8d8GTFfViap6H3gK2DU8oKpeqqp3B4cvA1u7zpUkrawuwb8FODl0fGrQdinfAH68xLmSpGW2YZRPluQOZoN/5xLm7gZ2A0xNTY2yLEnSkC5n/DPA9UPHWwdtH5Lk08ATwK6qemcxcwGqal9V9aqqNzk52aV2SdISdAn+w8D2JNuSbATuBw4OD0gyBTwNfLWqfraYuZKklbXgpZ6qOp/kQeAQMAHsr6pjSR4Y9O8FvgV8FPh+EoDzg7P3eecu01okSR2kqla7hov0er3q9/urXYYkrRtJjlRVr8tY37krSY0x+CWpMQa/JDXG4Jekxhj8ktQYg1+SGmPwS1JjDH5JaozBL0mNMfglqTEGvyQ1xuCXpMYY/JLUGINfkhpj8EtSYwx+SWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1JhOwZ/kniTHk0wn2TNP/01Jfprk90kentP3VpKjSV5N0h9V4ZKkpdmw0IAkE8DjwJ3AKeBwkoNV9cbQsF8D3wTuu8TT3FFVb19psZKkK9fljP82YLqqTlTV+8BTwK7hAVV1uqoOA+eWoUZJ0gh1Cf4twMmh41ODtq4KeCHJkSS7F1OcJGn0FrzUMwI7q2omyXXA80nerKoX5w4a/KWwG2BqamoFypKkNnU5458Brh863jpo66SqZgbfTwMHmL10NN+4fVXVq6re5ORk16eXJC1Sl+A/DGxPsi3JRuB+4GCXJ09ydZJrLjwG7gJeX2qxkqQrt+Clnqo6n+RB4BAwAeyvqmNJHhj0703ycaAPfAT4Q5KHgB3AZuBAkgs/68mqem55liJJ6qLTNf6qehZ4dk7b3qHH/8PsJaC5fgvcfCUFSpJGy3fuSlJjDH5JaozBL0mNMfglqTEGvyQ1xuCXpMYY/JLUGINfkhpj8EtSYwx+SWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1BiDX5IaY/BLUmMMfklqjMEvSY0x+CWpMQa/JDWmU/AnuSfJ8STTSfbM039Tkp8m+X2ShxczV5K0shYM/iQTwOPAvcAO4MtJdswZ9mvgm8C/LGGuJGkFdTnjvw2YrqoTVfU+8BSwa3hAVZ2uqsPAucXOlSStrC7BvwU4OXR8atDWRee5SXYn6SfpnzlzpuPTS5IWa828uFtV+6qqV1W9ycnJ1S5HksZWl+CfAa4fOt46aOviSuZKkpZBl+A/DGxPsi3JRuB+4GDH57+SuZKkZbBhoQFVdT7Jg8AhYALYX1XHkjww6N+b5ONAH/gI8IckDwE7quq3881drsVIkhaWqlrtGi7S6/Wq3++vdhmStG4kOVJVvS5j18yLu5KklWHwS1JjDH5JaozBL0mNMfglqTEGvyQ1xuCXpMYY/JLUGINfkhpj8EtSYwx+SWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1BiDX5IaY/BLUmMMfklqjMEvSY3pFPxJ7klyPMl0kj3z9CfJ9wb9ryW5dajvrSRHk7yaxE9Ql6RVtmGhAUkmgMeBO4FTwOEkB6vqjaFh9wLbB1+3Az8YfL/gjqp6e2RVS5KWrMsZ/23AdFWdqKr3gaeAXXPG7AJ+WLNeBq5N8okR1ypJGoEuwb8FODl0fGrQ1nVMAS8kOZJk91ILlSSNxoKXekZgZ1XNJLkOeD7Jm1X14txBg78UdgNMTU2tQFmS1KYuZ/wzwPVDx1sHbZ3GVNWF76eBA8xeOrpIVe2rql5V9SYnJ7tVL0latC7BfxjYnmRbko3A/cDBOWMOAl8b3N3zWeA3VfWrJFcnuQYgydXAXcDrI6xfkrRIC17qqarzSR4EDgETwP6qOpbkgUH/XuBZ4AvANPA74OuD6R8DDiS58LOerKrnRr4KSVJnqarVruEivV6v+n1v+ZekrpIcqapel7G+c1eSGmPwS1JjDH5JaozBL0mNMfglqTEGvyQ1xuCXpMYY/JLUGINfkhpj8EtSYwx+SWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1BiDX5IaY/BLUmMMfklqjMEvSY0x+CWpMZ2CP8k9SY4nmU6yZ57+JPneoP+1JLd2nbtWPfPKDJ/77k/YtudHfO67P+GZV2ZWuyRJGokNCw1IMgE8DtwJnAIOJzlYVW8MDbsX2D74uh34AXB7x7kjccOeH13U9q9f+gz33bJl0c/1zCszPPr0Uc6e+wCAmffO8ujTRwGW9HyStJZ0OeO/DZiuqhNV9T7wFLBrzphdwA9r1svAtUk+0XHuFZsv9AEe+vdXl3Sm/tih438M/QvOnvuAxw4dX1J9krSWdAn+LcDJoeNTg7YuY7rMXVZLCetfvnd2Ue2StJ6smRd3k+xO0k/SP3PmzMiedylh/clrNy2qXZLWky7BPwNcP3S8ddDWZUyXuQBU1b6q6lVVb3JyskNZ3SwlrB+5+0Y2XTXxobZNV03wyN03jqosSVo1XYL/MLA9ybYkG4H7gYNzxhwEvja4u+ezwG+q6lcd5y6rpYT1fbds4Ttf/BRbrt1EgC3XbuI7X/yUL+xKGgsL3tVTVeeTPAgcAiaA/VV1LMkDg/69wLPAF4Bp4HfA1y83d9SLeOu7fzfSu3pgNvwNeknjKFW12jVcpNfrVb/fX+0yJGndSHKkqnpdxq6ZF3clSSvD4Jekxhj8ktQYg1+SGmPwS1Jj1uRdPUnOAL9Y4vTNwNsjLGctco3jwTWOh7Wyxj+vqk7vfl2TwX8lkvS73tK0XrnG8eAax8N6XKOXeiSpMQa/JDVmHIN/32oXsAJc43hwjeNh3a1x7K7xS5IubxzP+CVJl7Fugj/J/iSnk7x+if51/4HvV7jGt5IcTfJqkjX7P9x1WONNSX6a5PdJHp7TNy77eLk1jss+fmXwZ/RokpeS3DzUNy77eLk1ru19rKp18QX8DXAr8Pol+r8A/BgI8FngPwftE8B/A38BbAT+C9ix2usZ5RoHfW8Bm1d7DSNY43XAXwH/DDw81D5O+zjvGsdsH/8a+LPB43vH9Pdx3jWuh31cN2f8VfUi8OvLDFnVD3wfhStY47qx0Bqr6nRVHQbOzekam328zBrXjQ5rfKmq3h0cvszsp+/BeO3jpda45q2b4O9gzX7g+whdbi0FvJDkSJLdK17Z8hunfbyccdzHbzD7L1UY330cXiOs8X1c8BO4tG7srKqZJNcBzyd5c3DGovVlrPYxyR3MhuLO1a5luVxijWt6H8fpjP+KP/B9HbjkWqrqwvfTwAFm/0k9TsZpHy9pnPYxyaeBJ4BdVfXOoHms9vESa1zz+zhOwb9mP/B9hOZdY5Krk1wDkORq4C5g3jsR1rFx2sd5jdM+JpkCnga+WlU/G+oam3281BrXwz6um0s9Sf4N+Ftgc5JTwD8BV8Ha+MD3UVjqGoGPAQeSwOyePllVz61o8R0ttMYkHwf6wEeAPyR5iNm7Pn47Lvt4qTUy+788jsU+At8CPgp8f7Ce81XVG7Pfx3nXyDr4ffSdu5LUmHG61CNJ6sDgl6TGGPyS1BiDX5IaY/BLUmMMfklqjMEvSY0x+CWpMf8H+LlMIDg6fEgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc01deda940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "hypothesis = X * W\n",
    "learning_rate = 0.1\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "gradient = tf.reduce_mean((W*X-Y)*X) *2\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train = optimizer.minimize(cost)\n",
    "gvs = optimizer.compute_gradients(cost)\n",
    "#train = optimizer.apply_gradients(gvs)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "W_val = []\n",
    "cost_val = []\n",
    "\n",
    "\n",
    "for i in range(20):\n",
    "    print(sess.run([gradient ,gvs], feed_dict={X:x_data, Y:y_data}))\n",
    "    sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
    "    W_val.append(sess.run(W))\n",
    "    cost_val.append(sess.run(cost, feed_dict={X:x_data, Y:y_data}))\n",
    "    \n",
    "print(W_val)\n",
    "print(cost_val)\n",
    "plt.plot(W_val,cost_val,'o')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
